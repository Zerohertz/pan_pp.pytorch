{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from mmcv import Config\n",
    "\n",
    "from dataset import build_data_loader\n",
    "from models import build_model\n",
    "from models.utils import fuse_module\n",
    "from utils import AverageMeter, Corrector, ResultFormat, Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model\": {\n",
      "        \"type\": \"PAN_PP\",\n",
      "        \"backbone\": {\n",
      "            \"type\": \"resnet50\",\n",
      "            \"pretrained\": true\n",
      "        },\n",
      "        \"neck\": {\n",
      "            \"type\": \"FPEM_v2\",\n",
      "            \"in_channels\": [\n",
      "                256,\n",
      "                512,\n",
      "                1024,\n",
      "                2048\n",
      "            ],\n",
      "            \"out_channels\": 128\n",
      "        },\n",
      "        \"detection_head\": {\n",
      "            \"type\": \"PAN_PP_DetHead\",\n",
      "            \"in_channels\": 512,\n",
      "            \"hidden_dim\": 128,\n",
      "            \"num_classes\": 6,\n",
      "            \"loss_text\": {\n",
      "                \"type\": \"DiceLoss\",\n",
      "                \"loss_weight\": 1.0\n",
      "            },\n",
      "            \"loss_kernel\": {\n",
      "                \"type\": \"DiceLoss\",\n",
      "                \"loss_weight\": 0.5\n",
      "            },\n",
      "            \"loss_emb\": {\n",
      "                \"type\": \"EmbLoss_v2\",\n",
      "                \"feature_dim\": 4,\n",
      "                \"loss_weight\": 0.25\n",
      "            },\n",
      "            \"use_coordconv\": false\n",
      "        }\n",
      "    },\n",
      "    \"data\": {\n",
      "        \"batch_size\": 1,\n",
      "        \"train\": {\n",
      "            \"type\": \"PAN_PP_TEST\",\n",
      "            \"split\": \"train\",\n",
      "            \"is_transform\": true,\n",
      "            \"img_size\": 736,\n",
      "            \"short_size\": 736,\n",
      "            \"kernel_scale\": 0.5,\n",
      "            \"read_type\": \"pil\",\n",
      "            \"with_rec\": false\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"type\": \"PAN_PP_TEST\",\n",
      "            \"split\": \"test\",\n",
      "            \"short_size\": 1024,\n",
      "            \"read_type\": \"cv2\",\n",
      "            \"with_rec\": false\n",
      "        }\n",
      "    },\n",
      "    \"train_cfg\": {\n",
      "        \"lr\": 0.001,\n",
      "        \"schedule\": \"polylr\",\n",
      "        \"epoch\": 200,\n",
      "        \"optimizer\": \"Adam\",\n",
      "        \"use_ex\": false\n",
      "    },\n",
      "    \"test_cfg\": {\n",
      "        \"min_score\": 0.75,\n",
      "        \"min_area\": 260,\n",
      "        \"min_kernel_area\": 0.1,\n",
      "        \"scale\": 2,\n",
      "        \"bbox_type\": \"rect\",\n",
      "        \"result_path\": \"outputs\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cfg = Config.fromfile('config/pan_pp/pan_pp_test.py')\n",
    "# for d in [cfg, cfg.data.test]:\n",
    "#     d.update(dict(report_speed=args.report_speed))\n",
    "# cfg.update(dict(vis=args.vis))\n",
    "# cfg.update(dict(debug=args.debug))\n",
    "# cfg.data.test.update(dict(debug=args.debug))\n",
    "print(json.dumps(cfg._cfg_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'PAN_PP_TEST',\n",
       " 'split': 'test',\n",
       " 'short_size': 1024,\n",
       " 'read_type': 'cv2',\n",
       " 'with_rec': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.data.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading type: cv2.\n"
     ]
    }
   ],
   "source": [
    "data_loader = build_data_loader(cfg.data.test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    data_loader,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(cfg.model, 'recognition_head'):\n",
    "    cfg.model.recognition_head.update(\n",
    "        dict(\n",
    "            voc=data_loader.voc,\n",
    "            char2id=data_loader.char2id,\n",
    "            id2char=data_loader.id2char,\n",
    "        ))\n",
    "model = build_model(cfg.model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and optimizer from checkpoint './weights/doc_panpp_best_weight.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "checkpoint = './weights/doc_panpp_best_weight.pth.tar'\n",
    "\n",
    "if checkpoint is not None:\n",
    "    if os.path.isfile(checkpoint):\n",
    "        print(\"Loading model and optimizer from checkpoint '{}'\".format(\n",
    "            checkpoint))\n",
    "\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "\n",
    "        d = dict()\n",
    "        for key, value in checkpoint['state_dict'].items():\n",
    "            tmp = key[7:]\n",
    "            d[tmp] = value\n",
    "        model.load_state_dict(d)\n",
    "    else:\n",
    "        print(\"No checkpoint found at '{}'\".format(args.resume))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "|           weight name          |               weight shape               |   number   |\n",
      "------------------------------------------------------------------------------------------\n",
      "| backbone.conv1.weight          | torch.Size([64, 3, 3, 3])                | 1728       |\n",
      "| backbone.conv1.bias            | torch.Size([64])                         | 64         |\n",
      "| backbone.conv2.weight          | torch.Size([64, 64, 3, 3])               | 36864      |\n",
      "| backbone.conv2.bias            | torch.Size([64])                         | 64         |\n",
      "| backbone.conv3.weight          | torch.Size([128, 64, 3, 3])              | 73728      |\n",
      "| backbone.conv3.bias            | torch.Size([128])                        | 128        |\n",
      "| backbone.layer1.0.conv1.weight | torch.Size([64, 128, 1, 1])              | 8192       |\n",
      "| backbone.layer1.0.conv1.bias   | torch.Size([64])                         | 64         |\n",
      "| backbone.layer1.0.conv2.weight | torch.Size([64, 64, 3, 3])               | 36864      |\n",
      "| backbone.layer1.0.conv2.bias   | torch.Size([64])                         | 64         |\n",
      "| backbone.layer1.0.conv3.weight | torch.Size([256, 64, 1, 1])              | 16384      |\n",
      "| backbone.layer1.0.conv3.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer1.0.downsample.0.weight | torch.Size([256, 128, 1, 1])             | 32768      |\n",
      "| backbone.layer1.0.downsample.0.bias | torch.Size([256])                        | 256        |\n",
      "| backbone.layer1.1.conv1.weight | torch.Size([64, 256, 1, 1])              | 16384      |\n",
      "| backbone.layer1.1.conv1.bias   | torch.Size([64])                         | 64         |\n",
      "| backbone.layer1.1.conv2.weight | torch.Size([64, 64, 3, 3])               | 36864      |\n",
      "| backbone.layer1.1.conv2.bias   | torch.Size([64])                         | 64         |\n",
      "| backbone.layer1.1.conv3.weight | torch.Size([256, 64, 1, 1])              | 16384      |\n",
      "| backbone.layer1.1.conv3.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer1.2.conv1.weight | torch.Size([64, 256, 1, 1])              | 16384      |\n",
      "| backbone.layer1.2.conv1.bias   | torch.Size([64])                         | 64         |\n",
      "| backbone.layer1.2.conv2.weight | torch.Size([64, 64, 3, 3])               | 36864      |\n",
      "| backbone.layer1.2.conv2.bias   | torch.Size([64])                         | 64         |\n",
      "| backbone.layer1.2.conv3.weight | torch.Size([256, 64, 1, 1])              | 16384      |\n",
      "| backbone.layer1.2.conv3.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer2.0.conv1.weight | torch.Size([128, 256, 1, 1])             | 32768      |\n",
      "| backbone.layer2.0.conv1.bias   | torch.Size([128])                        | 128        |\n",
      "| backbone.layer2.0.conv2.weight | torch.Size([128, 128, 3, 3])             | 147456     |\n",
      "| backbone.layer2.0.conv2.bias   | torch.Size([128])                        | 128        |\n",
      "| backbone.layer2.0.conv3.weight | torch.Size([512, 128, 1, 1])             | 65536      |\n",
      "| backbone.layer2.0.conv3.bias   | torch.Size([512])                        | 512        |\n",
      "| backbone.layer2.0.downsample.0.weight | torch.Size([512, 256, 1, 1])             | 131072     |\n",
      "| backbone.layer2.0.downsample.0.bias | torch.Size([512])                        | 512        |\n",
      "| backbone.layer2.1.conv1.weight | torch.Size([128, 512, 1, 1])             | 65536      |\n",
      "| backbone.layer2.1.conv1.bias   | torch.Size([128])                        | 128        |\n",
      "| backbone.layer2.1.conv2.weight | torch.Size([128, 128, 3, 3])             | 147456     |\n",
      "| backbone.layer2.1.conv2.bias   | torch.Size([128])                        | 128        |\n",
      "| backbone.layer2.1.conv3.weight | torch.Size([512, 128, 1, 1])             | 65536      |\n",
      "| backbone.layer2.1.conv3.bias   | torch.Size([512])                        | 512        |\n",
      "| backbone.layer2.2.conv1.weight | torch.Size([128, 512, 1, 1])             | 65536      |\n",
      "| backbone.layer2.2.conv1.bias   | torch.Size([128])                        | 128        |\n",
      "| backbone.layer2.2.conv2.weight | torch.Size([128, 128, 3, 3])             | 147456     |\n",
      "| backbone.layer2.2.conv2.bias   | torch.Size([128])                        | 128        |\n",
      "| backbone.layer2.2.conv3.weight | torch.Size([512, 128, 1, 1])             | 65536      |\n",
      "| backbone.layer2.2.conv3.bias   | torch.Size([512])                        | 512        |\n",
      "| backbone.layer2.3.conv1.weight | torch.Size([128, 512, 1, 1])             | 65536      |\n",
      "| backbone.layer2.3.conv1.bias   | torch.Size([128])                        | 128        |\n",
      "| backbone.layer2.3.conv2.weight | torch.Size([128, 128, 3, 3])             | 147456     |\n",
      "| backbone.layer2.3.conv2.bias   | torch.Size([128])                        | 128        |\n",
      "| backbone.layer2.3.conv3.weight | torch.Size([512, 128, 1, 1])             | 65536      |\n",
      "| backbone.layer2.3.conv3.bias   | torch.Size([512])                        | 512        |\n",
      "| backbone.layer3.0.conv1.weight | torch.Size([256, 512, 1, 1])             | 131072     |\n",
      "| backbone.layer3.0.conv1.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer3.0.conv2.weight | torch.Size([256, 256, 3, 3])             | 589824     |\n",
      "| backbone.layer3.0.conv2.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer3.0.conv3.weight | torch.Size([1024, 256, 1, 1])            | 262144     |\n",
      "| backbone.layer3.0.conv3.bias   | torch.Size([1024])                       | 1024       |\n",
      "| backbone.layer3.0.downsample.0.weight | torch.Size([1024, 512, 1, 1])            | 524288     |\n",
      "| backbone.layer3.0.downsample.0.bias | torch.Size([1024])                       | 1024       |\n",
      "| backbone.layer3.1.conv1.weight | torch.Size([256, 1024, 1, 1])            | 262144     |\n",
      "| backbone.layer3.1.conv1.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer3.1.conv2.weight | torch.Size([256, 256, 3, 3])             | 589824     |\n",
      "| backbone.layer3.1.conv2.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer3.1.conv3.weight | torch.Size([1024, 256, 1, 1])            | 262144     |\n",
      "| backbone.layer3.1.conv3.bias   | torch.Size([1024])                       | 1024       |\n",
      "| backbone.layer3.2.conv1.weight | torch.Size([256, 1024, 1, 1])            | 262144     |\n",
      "| backbone.layer3.2.conv1.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer3.2.conv2.weight | torch.Size([256, 256, 3, 3])             | 589824     |\n",
      "| backbone.layer3.2.conv2.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer3.2.conv3.weight | torch.Size([1024, 256, 1, 1])            | 262144     |\n",
      "| backbone.layer3.2.conv3.bias   | torch.Size([1024])                       | 1024       |\n",
      "| backbone.layer3.3.conv1.weight | torch.Size([256, 1024, 1, 1])            | 262144     |\n",
      "| backbone.layer3.3.conv1.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer3.3.conv2.weight | torch.Size([256, 256, 3, 3])             | 589824     |\n",
      "| backbone.layer3.3.conv2.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer3.3.conv3.weight | torch.Size([1024, 256, 1, 1])            | 262144     |\n",
      "| backbone.layer3.3.conv3.bias   | torch.Size([1024])                       | 1024       |\n",
      "| backbone.layer3.4.conv1.weight | torch.Size([256, 1024, 1, 1])            | 262144     |\n",
      "| backbone.layer3.4.conv1.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer3.4.conv2.weight | torch.Size([256, 256, 3, 3])             | 589824     |\n",
      "| backbone.layer3.4.conv2.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer3.4.conv3.weight | torch.Size([1024, 256, 1, 1])            | 262144     |\n",
      "| backbone.layer3.4.conv3.bias   | torch.Size([1024])                       | 1024       |\n",
      "| backbone.layer3.5.conv1.weight | torch.Size([256, 1024, 1, 1])            | 262144     |\n",
      "| backbone.layer3.5.conv1.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer3.5.conv2.weight | torch.Size([256, 256, 3, 3])             | 589824     |\n",
      "| backbone.layer3.5.conv2.bias   | torch.Size([256])                        | 256        |\n",
      "| backbone.layer3.5.conv3.weight | torch.Size([1024, 256, 1, 1])            | 262144     |\n",
      "| backbone.layer3.5.conv3.bias   | torch.Size([1024])                       | 1024       |\n",
      "| backbone.layer4.0.conv1.weight | torch.Size([512, 1024, 1, 1])            | 524288     |\n",
      "| backbone.layer4.0.conv1.bias   | torch.Size([512])                        | 512        |\n",
      "| backbone.layer4.0.conv2.weight | torch.Size([512, 512, 3, 3])             | 2359296    |\n",
      "| backbone.layer4.0.conv2.bias   | torch.Size([512])                        | 512        |\n",
      "| backbone.layer4.0.conv3.weight | torch.Size([2048, 512, 1, 1])            | 1048576    |\n",
      "| backbone.layer4.0.conv3.bias   | torch.Size([2048])                       | 2048       |\n",
      "| backbone.layer4.0.downsample.0.weight | torch.Size([2048, 1024, 1, 1])           | 2097152    |\n",
      "| backbone.layer4.0.downsample.0.bias | torch.Size([2048])                       | 2048       |\n",
      "| backbone.layer4.1.conv1.weight | torch.Size([512, 2048, 1, 1])            | 1048576    |\n",
      "| backbone.layer4.1.conv1.bias   | torch.Size([512])                        | 512        |\n",
      "| backbone.layer4.1.conv2.weight | torch.Size([512, 512, 3, 3])             | 2359296    |\n",
      "| backbone.layer4.1.conv2.bias   | torch.Size([512])                        | 512        |\n",
      "| backbone.layer4.1.conv3.weight | torch.Size([2048, 512, 1, 1])            | 1048576    |\n",
      "| backbone.layer4.1.conv3.bias   | torch.Size([2048])                       | 2048       |\n",
      "| backbone.layer4.2.conv1.weight | torch.Size([512, 2048, 1, 1])            | 1048576    |\n",
      "| backbone.layer4.2.conv1.bias   | torch.Size([512])                        | 512        |\n",
      "| backbone.layer4.2.conv2.weight | torch.Size([512, 512, 3, 3])             | 2359296    |\n",
      "| backbone.layer4.2.conv2.bias   | torch.Size([512])                        | 512        |\n",
      "| backbone.layer4.2.conv3.weight | torch.Size([2048, 512, 1, 1])            | 1048576    |\n",
      "| backbone.layer4.2.conv3.bias   | torch.Size([2048])                       | 2048       |\n",
      "| reduce_layer4.conv.weight      | torch.Size([128, 2048, 1, 1])            | 262144     |\n",
      "| reduce_layer4.conv.bias        | torch.Size([128])                        | 128        |\n",
      "| reduce_layer3.conv.weight      | torch.Size([128, 1024, 1, 1])            | 131072     |\n",
      "| reduce_layer3.conv.bias        | torch.Size([128])                        | 128        |\n",
      "| reduce_layer2.conv.weight      | torch.Size([128, 512, 1, 1])             | 65536      |\n",
      "| reduce_layer2.conv.bias        | torch.Size([128])                        | 128        |\n",
      "| reduce_layer1.conv.weight      | torch.Size([128, 256, 1, 1])             | 32768      |\n",
      "| reduce_layer1.conv.bias        | torch.Size([128])                        | 128        |\n",
      "| fpem1.dwconv3_1.weight         | torch.Size([128, 1, 3, 3])               | 1152       |\n",
      "| fpem1.smooth_layer3_1.conv.weight | torch.Size([128, 128, 1, 1])             | 16384      |\n",
      "| fpem1.smooth_layer3_1.conv.bias | torch.Size([128])                        | 128        |\n",
      "| fpem1.dwconv2_1.weight         | torch.Size([128, 1, 3, 3])               | 1152       |\n",
      "| fpem1.smooth_layer2_1.conv.weight | torch.Size([128, 128, 1, 1])             | 16384      |\n",
      "| fpem1.smooth_layer2_1.conv.bias | torch.Size([128])                        | 128        |\n",
      "| fpem1.dwconv1_1.weight         | torch.Size([128, 1, 3, 3])               | 1152       |\n",
      "| fpem1.smooth_layer1_1.conv.weight | torch.Size([128, 128, 1, 1])             | 16384      |\n",
      "| fpem1.smooth_layer1_1.conv.bias | torch.Size([128])                        | 128        |\n",
      "| fpem1.dwconv2_2.weight         | torch.Size([128, 1, 3, 3])               | 1152       |\n",
      "| fpem1.smooth_layer2_2.conv.weight | torch.Size([128, 128, 1, 1])             | 16384      |\n",
      "| fpem1.smooth_layer2_2.conv.bias | torch.Size([128])                        | 128        |\n",
      "| fpem1.dwconv3_2.weight         | torch.Size([128, 1, 3, 3])               | 1152       |\n",
      "| fpem1.smooth_layer3_2.conv.weight | torch.Size([128, 128, 1, 1])             | 16384      |\n",
      "| fpem1.smooth_layer3_2.conv.bias | torch.Size([128])                        | 128        |\n",
      "| fpem1.dwconv4_2.weight         | torch.Size([128, 1, 3, 3])               | 1152       |\n",
      "| fpem1.smooth_layer4_2.conv.weight | torch.Size([128, 128, 1, 1])             | 16384      |\n",
      "| fpem1.smooth_layer4_2.conv.bias | torch.Size([128])                        | 128        |\n",
      "| fpem2.dwconv3_1.weight         | torch.Size([128, 1, 3, 3])               | 1152       |\n",
      "| fpem2.smooth_layer3_1.conv.weight | torch.Size([128, 128, 1, 1])             | 16384      |\n",
      "| fpem2.smooth_layer3_1.conv.bias | torch.Size([128])                        | 128        |\n",
      "| fpem2.dwconv2_1.weight         | torch.Size([128, 1, 3, 3])               | 1152       |\n",
      "| fpem2.smooth_layer2_1.conv.weight | torch.Size([128, 128, 1, 1])             | 16384      |\n",
      "| fpem2.smooth_layer2_1.conv.bias | torch.Size([128])                        | 128        |\n",
      "| fpem2.dwconv1_1.weight         | torch.Size([128, 1, 3, 3])               | 1152       |\n",
      "| fpem2.smooth_layer1_1.conv.weight | torch.Size([128, 128, 1, 1])             | 16384      |\n",
      "| fpem2.smooth_layer1_1.conv.bias | torch.Size([128])                        | 128        |\n",
      "| fpem2.dwconv2_2.weight         | torch.Size([128, 1, 3, 3])               | 1152       |\n",
      "| fpem2.smooth_layer2_2.conv.weight | torch.Size([128, 128, 1, 1])             | 16384      |\n",
      "| fpem2.smooth_layer2_2.conv.bias | torch.Size([128])                        | 128        |\n",
      "| fpem2.dwconv3_2.weight         | torch.Size([128, 1, 3, 3])               | 1152       |\n",
      "| fpem2.smooth_layer3_2.conv.weight | torch.Size([128, 128, 1, 1])             | 16384      |\n",
      "| fpem2.smooth_layer3_2.conv.bias | torch.Size([128])                        | 128        |\n",
      "| fpem2.dwconv4_2.weight         | torch.Size([128, 1, 3, 3])               | 1152       |\n",
      "| fpem2.smooth_layer4_2.conv.weight | torch.Size([128, 128, 1, 1])             | 16384      |\n",
      "| fpem2.smooth_layer4_2.conv.bias | torch.Size([128])                        | 128        |\n",
      "| det_head.conv1.weight          | torch.Size([128, 512, 3, 3])             | 589824     |\n",
      "| det_head.conv1.bias            | torch.Size([128])                        | 128        |\n",
      "| det_head.conv2.weight          | torch.Size([6, 128, 1, 1])               | 768        |\n",
      "| det_head.conv2.bias            | torch.Size([6])                          | 6          |\n",
      "------------------------------------------------------------------------------------------\n",
      "The total number of parameters: 24899782\n",
      "The parameters of Model PAN_PP: 24.899782M\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = fuse_module(model)\n",
    "\n",
    "def model_structure(model):\n",
    "    blank = ' '\n",
    "    print('-' * 90)\n",
    "    print('|' + ' ' * 11 + 'weight name' + ' ' * 10 + '|' \\\n",
    "          + ' ' * 15 + 'weight shape' + ' ' * 15 + '|' \\\n",
    "          + ' ' * 3 + 'number' + ' ' * 3 + '|')\n",
    "    print('-' * 90)\n",
    "    num_para = 0\n",
    "\n",
    "    for index, (key, w_variable) in enumerate(model.named_parameters()):\n",
    "        if len(key) <= 30:\n",
    "            key = key + (30 - len(key)) * blank\n",
    "        shape = str(w_variable.shape)\n",
    "        if len(shape) <= 40:\n",
    "            shape = shape + (40 - len(shape)) * blank\n",
    "        each_para = 1\n",
    "        for k in w_variable.shape:\n",
    "            each_para *= k\n",
    "        num_para += each_para\n",
    "        str_num = str(each_para)\n",
    "        if len(str_num) <= 10:\n",
    "            str_num = str_num + (10 - len(str_num)) * blank\n",
    "\n",
    "        print('| {} | {} | {} |'.format(key, shape, str_num))\n",
    "    print('-' * 90)\n",
    "    print('The total number of parameters: ' + str(num_para))\n",
    "    print('The parameters of Model {}: {:4f}M'.format(\n",
    "        model._get_name(), num_para / 1e6))\n",
    "    print('-' * 90)\n",
    "    \n",
    "\n",
    "model_structure(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing 101 images\n",
      "Testing 0/101\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:3060: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing 1/101\n",
      "Testing 2/101\n",
      "Testing 3/101\n",
      "Testing 4/101\n",
      "Testing 5/101\n",
      "Testing 6/101\n",
      "Testing 7/101\n",
      "Testing 8/101\n",
      "Testing 9/101\n",
      "Testing 10/101\n",
      "Testing 11/101\n",
      "Testing 12/101\n",
      "Testing 13/101\n",
      "Testing 14/101\n",
      "Testing 15/101\n",
      "Testing 16/101\n",
      "Testing 17/101\n",
      "Testing 18/101\n",
      "Testing 19/101\n",
      "Testing 20/101\n",
      "Testing 21/101\n",
      "Testing 22/101\n",
      "Testing 23/101\n",
      "Testing 24/101\n",
      "Testing 25/101\n",
      "Testing 26/101\n",
      "Testing 27/101\n",
      "Testing 28/101\n",
      "Testing 29/101\n",
      "Testing 30/101\n",
      "Testing 31/101\n",
      "Testing 32/101\n",
      "Testing 33/101\n",
      "Testing 34/101\n",
      "Testing 35/101\n",
      "Testing 36/101\n",
      "Testing 37/101\n",
      "Testing 38/101\n",
      "Testing 39/101\n",
      "Testing 40/101\n",
      "Testing 41/101\n",
      "Testing 42/101\n",
      "Testing 43/101\n",
      "Testing 44/101\n",
      "Testing 45/101\n",
      "Testing 46/101\n",
      "Testing 47/101\n",
      "Testing 48/101\n",
      "Testing 49/101\n",
      "Testing 50/101\n",
      "Testing 51/101\n",
      "Testing 52/101\n",
      "Testing 53/101\n",
      "Testing 54/101\n",
      "Testing 55/101\n",
      "Testing 56/101\n",
      "Testing 57/101\n",
      "Testing 58/101\n",
      "Testing 59/101\n",
      "Testing 60/101\n",
      "Testing 61/101\n",
      "Testing 62/101\n",
      "Testing 63/101\n",
      "Testing 64/101\n",
      "Testing 65/101\n",
      "Testing 66/101\n",
      "Testing 67/101\n",
      "Testing 68/101\n",
      "Testing 69/101\n",
      "Testing 70/101\n",
      "Testing 71/101\n",
      "Testing 72/101\n",
      "Testing 73/101\n",
      "Testing 74/101\n",
      "Testing 75/101\n",
      "Testing 76/101\n",
      "Testing 77/101\n",
      "Testing 78/101\n",
      "Testing 79/101\n",
      "Testing 80/101\n",
      "Testing 81/101\n",
      "Testing 82/101\n",
      "Testing 83/101\n",
      "Testing 84/101\n",
      "Testing 85/101\n",
      "Testing 86/101\n",
      "Testing 87/101\n",
      "Testing 88/101\n",
      "Testing 89/101\n",
      "Testing 90/101\n",
      "Testing 91/101\n",
      "Testing 92/101\n",
      "Testing 93/101\n",
      "Testing 94/101\n",
      "Testing 95/101\n",
      "Testing 96/101\n",
      "Testing 97/101\n",
      "Testing 98/101\n",
      "Testing 99/101\n",
      "Testing 100/101\n",
      "Clear\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with_rec = hasattr(cfg.model, 'recognition_head')\n",
    "\n",
    "rf = ResultFormat(cfg.data.test.type, cfg.test_cfg.result_path)\n",
    "print('Start testing %d images' % len(test_loader))\n",
    "cfg.debug = False\n",
    "cfg.report_speed = False\n",
    "\n",
    "for idx, data in enumerate(test_loader):\n",
    "    print('Testing %d/%d\\r' % (idx, len(test_loader)), end='', flush=True)\n",
    "    \n",
    "    # prepare input\n",
    "    data['imgs'] = data['imgs'].cuda()\n",
    "    data.update(dict(cfg=cfg))\n",
    "\n",
    "    # forward\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**data)\n",
    "\n",
    "    # save result\n",
    "    image_name, _ = osp.splitext(osp.basename(test_loader.dataset.img_paths[idx]))\n",
    "    image_path = test_loader.dataset.img_paths[idx]\n",
    "    rf.write_result(image_name, image_path, outputs)\n",
    "#     rf.write_result(data['img_metas'], outputs)\n",
    "\n",
    "print('Clear')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
